#---- Tokenizing the data
Training on 11 MIDI files.


Tokenizing MIDIs (Maestro_tokens_no_bpe):  91%|█████████ | 10/11 [00:05<00:00,  1.58it/s]
Tokenizing MIDIs (Maestro_tokens_no_bpe): 100%|██████████| 11/11 [00:05<00:00,  1.86it/s]
Loading data: Maestro_tokens_no_bpe: 100%|██████████| 11/11 [00:00<00:00, 146.97it/s]
max_steps is given, it will override any value given in num_train_epochs
***** Running training *****
  Num examples = 329
  Num Epochs = 14,286
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 48
  Gradient Accumulation steps = 3
  Total optimization steps = 100,000
  Number of trainable parameters = 26,488,320
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/100000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
#---- Starting Training


  0%|          | 3/100000 [04:17<2383:24:12, 85.81s/it]Traceback (most recent call last):
  File "/Users/jlenz/Desktop/Qosmo/symbolic-models/GPT2/main.py", line 166, in <module>
    train_result = trainer.train()
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 1668, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 1940, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 2753, in training_step
    loss.backward()
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/autograd/function.py", line 267, in apply
    return user_fn(self, *args)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 141, in backward
    outputs = ctx.run_function(*detached_inputs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 886, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 396, in forward
    output_attentions=output_attentions,
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 312, in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/pytorch_utils.py", line 102, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
