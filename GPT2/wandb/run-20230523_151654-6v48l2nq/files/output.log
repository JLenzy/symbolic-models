
  0%|                                                                                                                                                             | 0/100000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Traceback (most recent call last):
  File "GPT2.py", line 125, in <module>
    train_result = trainer.train()
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 1668, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 1940, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 2753, in training_step
    loss.backward()
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/autograd/function.py", line 267, in apply
    return user_fn(self, *args)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 141, in backward
    outputs = ctx.run_function(*detached_inputs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 886, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 396, in forward
    output_attentions=output_attentions,
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 331, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 191, in _attn
    if self.scale_attn_by_inverse_layer_idx:
KeyboardInterrupt