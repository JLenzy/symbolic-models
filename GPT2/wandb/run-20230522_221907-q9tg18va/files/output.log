#---- Tokenizing the data
Training on 11 MIDI files.


Tokenizing MIDIs (Maestro_tokens_no_bpe):  91%|█████████ | 10/11 [00:05<00:00,  1.54it/s]
Tokenizing MIDIs (Maestro_tokens_no_bpe): 100%|██████████| 11/11 [00:06<00:00,  1.77it/s]
Loading data: Maestro_tokens_no_bpe: 100%|██████████| 11/11 [00:00<00:00, 175.48it/s]
#---- Starting Training
max_steps is given, it will override any value given in num_train_epochs
***** Running training *****
  Num examples = 308
  Num Epochs = 16,667
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 48
  Gradient Accumulation steps = 3
  Total optimization steps = 100,000
  Number of trainable parameters = 45,150,948
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/100000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...


















  0%|          | 19/100000 [38:38<3355:33:53, 120.82s/it]




















  0%|          | 39/100000 [1:17:50<3337:28:12, 120.20s/it]




















  0%|          | 59/100000 [1:56:56<3336:57:09, 120.20s/it]




















  0%|          | 79/100000 [2:36:03<3325:57:13, 119.83s/it]



