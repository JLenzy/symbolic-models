Loading data: tokenized_data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 264.84it/s]
max_steps is given, it will override any value given in num_train_epochs
***** Running training *****
  Num examples = 305
  Num Epochs = 16,667
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 48
  Gradient Accumulation steps = 3
  Total optimization steps = 100,000
  Number of trainable parameters = 26,409,472
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                                                                             | 0/100000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
length of vocab: 275
#---- Loading the Model
#---- Starting Training
Traceback (most recent call last):
  File "GPT2.py", line 118, in <module>
    train_result = trainer.train()
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 1668, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 1940, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/trainer.py", line 2753, in training_step
    loss.backward()
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/autograd/function.py", line 267, in apply
    return user_fn(self, *args)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 141, in backward
    outputs = ctx.run_function(*detached_inputs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 886, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 396, in forward
    output_attentions=output_attentions,
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 331, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/Users/jlenz/opt/miniconda3/envs/qosmo/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 218, in _attn
    attn_output = torch.matmul(attn_weights, value)
KeyboardInterrupt